{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acff6ec8-6b59-416f-aa97-2f77be08a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import slicer\n",
    "import vtk\n",
    "import slicer_utils\n",
    "import utils\n",
    "import trim_utils\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62ddf50-da34-4fcb-9271-4ee7cf1e523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vtpfile(filename):\n",
    "    reader = vtk.vtkXMLPolyDataReader()\n",
    "    reader.SetFileName(filename)\n",
    "    reader.Update()\n",
    "    return reader.GetOutput()\n",
    "\n",
    "def write_vtpfile(polydata, filename):\n",
    "    writer = vtk.vtkXMLPolyDataWriter()\n",
    "    writer.SetFileName(filename)\n",
    "    writer.SetInputData(polydata)\n",
    "    writer.Write()\n",
    "\n",
    "def write_vtkfile(polydata, filename):\n",
    "    vtk_writer = vtk.vtkPolyDataWriter()\n",
    "    vtk_writer.SetFileName(filename)\n",
    "    vtk_writer.SetInputData(polydata)\n",
    "    #vtk_writer.SetFileVersion(42)\n",
    "    vtk_writer.Write()\n",
    "\n",
    "def read_vtkfile(filename):\n",
    "    reader = vtk.vtkPolyDataReader()\n",
    "    reader.SetFileName(filename)\n",
    "    reader.Update()\n",
    "    return reader.GetOutput()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866c148a-54e0-4667-be28-83e1f59a00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pa_tree(file_name):\n",
    "    # Input: file_name = 'base_path/ID.nii.gz'\n",
    "    # Saves centerline, network, preprocessed, endpoints to  'base_path/vmtk_out/'\n",
    "    base_name = os.path.basename(file_name)\n",
    "    ID = base_name.split('.nii.gz')[0]\n",
    "    base_path = file_name.split(ID)[0]\n",
    "    \n",
    "    save_path = os.path.join(base_path,'vmtk_out_pa')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    mesh_file_path = f\"{save_path}/{ID}_mesh.vtk\"\n",
    "\n",
    "    loadedVolume = slicer.util.loadVolume(file_name)\n",
    "\n",
    "    labelmapVolumeNode = slicer.mrmlScene.AddNewNodeByClass(\"vtkMRMLLabelMapVolumeNode\")\n",
    "    slicer.modules.volumes.logic().CreateLabelVolumeFromVolume(slicer.mrmlScene, labelmapVolumeNode, loadedVolume)\n",
    "\n",
    "    # Step 3: Create a new segmentation node and import the labelmap into the segmentation node\n",
    "    segmentationNode = slicer.mrmlScene.AddNewNodeByClass(\"vtkMRMLSegmentationNode\")\n",
    "    slicer.modules.segmentations.logic().ImportLabelmapToSegmentationNode(labelmapVolumeNode, segmentationNode)\n",
    "\n",
    "    segmentIDs = vtk.vtkStringArray()\n",
    "    segmentationNode.GetSegmentation().GetSegmentIDs(segmentIDs)\n",
    "\n",
    "    #Create desired geometryImageData with overSamplingFactor\n",
    "    segmentationGeometryLogic = slicer.vtkSlicerSegmentationGeometryLogic()\n",
    "    segmentationGeometryLogic.SetInputSegmentationNode(segmentationNode)\n",
    "    segmentationGeometryLogic.SetSourceGeometryNode(segmentationNode)\n",
    "    segmentationGeometryLogic.SetOversamplingFactor(2)\n",
    "    segmentationGeometryLogic.CalculateOutputGeometry()\n",
    "    geometryImageData = segmentationGeometryLogic.GetOutputGeometryImageData()\n",
    "\n",
    "    for index in range(segmentIDs.GetNumberOfValues()):\n",
    "        currentSegmentID = segmentIDs.GetValue(index)\n",
    "        currentSegment = segmentationNode.GetSegmentation().GetSegment(currentSegmentID)\n",
    "        currentLabelmap = currentSegment.GetRepresentation(\"Binary labelmap\")\n",
    "    \n",
    "        success = slicer.vtkOrientedImageDataResample.ResampleOrientedImageToReferenceOrientedImage(currentLabelmap, geometryImageData, currentLabelmap, False, True)\n",
    "    \n",
    "        if not success:\n",
    "                print(\"Segment {}/{} failed to be resampled\".format(segmentationNode.GetName(), currentSegmentID))\n",
    "\n",
    "    segmentationNode.Modified()\n",
    "    segmentationNode.CreateClosedSurfaceRepresentation()\n",
    "\n",
    "    segmentId = segmentationNode.GetSegmentation().GetNthSegmentID(0)  # Get the first segment's ID (or modify for specific segments)\n",
    "    vtk_mesh = segmentationNode.GetSegmentation().GetSegment(segmentId).GetRepresentation(slicer.vtkSegmentationConverter.GetClosedSurfaceRepresentationName())\n",
    "    logic = utils.ExtractCenterlineLogic()\n",
    "    \n",
    "    # Default preprocessing parameter\n",
    "    targetpts = float(5000)\n",
    "    decimationAgressiveness = float(4.0)\n",
    "    subDivideInput = False\n",
    "   \n",
    "    preProcess = logic.preprocess(vtk_mesh, targetpts, decimationAgressiveness, subDivideInput)\n",
    "    \n",
    "    retain_connected_component = True\n",
    "    if retain_connected_component:\n",
    "        connectivityFilter = vtk.vtkConnectivityFilter()\n",
    "        connectivityFilter.SetInputData(preProcess)\n",
    "        connectivityFilter.SetExtractionModeToLargestRegion()  # Keep only the largest connected component\n",
    "        connectivityFilter.Update()\n",
    "        preProcess = connectivityFilter.GetOutput()\n",
    "\n",
    "    endPointsMarkupsNode = slicer.mrmlScene.AddNewNodeByClass(\"vtkMRMLMarkupsFiducialNode\", \"Centerline endpoints\")\n",
    "    networkPolyData = logic.extractNetwork(preProcess, endPointsMarkupsNode, computeGeometry=True)\n",
    "\n",
    "    #########\n",
    "    startPointPosition=None\n",
    "    endpointPositions = logic.getEndPoints(networkPolyData, startPointPosition)\n",
    "    endPointsMarkupsNode.GetDisplayNode().PointLabelsVisibilityOff()\n",
    "    endPointsMarkupsNode.RemoveAllMarkups()\n",
    "\n",
    "    \n",
    "    for position in endpointPositions:\n",
    "        endPointsMarkupsNode.AddControlPoint(vtk.vtkVector3d(position))\n",
    "\n",
    "    if endPointsMarkupsNode.GetNumberOfControlPoints() > 0:\n",
    "        endPointsMarkupsNode.SetNthControlPointSelected(0, False)\n",
    "    ##########\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    if not endPointsMarkupsNode or endPointsMarkupsNode.GetNumberOfControlPoints() < 2:\n",
    "        #raise ValueError(\"At least two endpoints are needed for centerline extraction\")\n",
    "        print('*****Two end points are needed for centerline extraction!!!******')\n",
    "        print(f'Skipping ID = {ID}. Best to do manually.')\n",
    "        slicer.mrmlScene.RemoveNode(loadedVolume)\n",
    "        slicer.mrmlScene.RemoveNode(labelmapVolumeNode)\n",
    "        slicer.mrmlScene.RemoveNode(segmentationNode)\n",
    "        slicer.mrmlScene.RemoveNode(endPointsMarkupsNode)\n",
    "\n",
    "        return 0\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    base_name = os.path.basename(file_name)\n",
    "    ID = base_name.split('.nii.gz')[0]\n",
    "    base_path = file_name.split(ID)[0]\n",
    "\n",
    "    # Step 6: Save the data\n",
    "    vtk_writer = vtk.vtkPolyDataWriter()\n",
    "\n",
    "    #mesh_file_path = f\"{save_path}/{ID}_mesh.vtk\"\n",
    "    #vtk_writer.SetFileName(mesh_file_path)\n",
    "    #vtk_writer.SetInputData(vtk_mesh)\n",
    "    #vtk_writer.SetFileVersion(42)\n",
    "    #vtk_writer.Write()\n",
    "    #print(f\"Saved network data to {mesh_file_path}\")\n",
    "\n",
    "    # Save networkPolyData as vtk\n",
    "    network_file_path = f\"{save_path}/{ID}_network.vtp\"\n",
    "    write_vtpfile(networkPolyData,network_file_path)\n",
    "    print(f\"Saved network data to {network_file_path}\")\n",
    "\n",
    "    # Save endPointsMarkupsNode as fcsv (fiducial list)\n",
    "    endpoints_file_path = f\"{save_path}/{ID}_endpoints.fcsv\"\n",
    "    slicer.util.saveNode(endPointsMarkupsNode, endpoints_file_path)\n",
    "    print(f\"Saved endpoints to {endpoints_file_path}\")\n",
    "\n",
    "    preprocess_file_path = f\"{save_path}/{ID}_meshPreProcessed.vtk\"\n",
    "    vtk_writer.SetFileName(preprocess_file_path)\n",
    "    vtk_writer.SetInputData(preProcess)\n",
    "    vtk_writer.SetFileVersion(42)\n",
    "    vtk_writer.Write()\n",
    "    print(f\"Saved network data to {preprocess_file_path}\")\n",
    "\n",
    "    # Save centerlinePolyData as vtk\n",
    "    \n",
    "\n",
    "    do_centerlines = False\n",
    "    if do_centerlines:\n",
    "        centerlinePolyData, voronoiDiagramPolyData = logic.extractCenterline(preProcess, endPointsMarkupsNode)\n",
    "        centerlinePropertiesTableNode = None\n",
    "    \n",
    "        centerline_file_path = f\"{save_path}/{ID}_centerline.vtk\"\n",
    "        vtk_writer.SetFileName(centerline_file_path)\n",
    "        vtk_writer.SetInputData(centerlinePolyData)\n",
    "        vtk_writer.SetFileVersion(42)\n",
    "        vtk_writer.Write()\n",
    "        print(f\"Saved centerline data to {centerline_file_path}\")\n",
    "\n",
    "        snap_endpoints = False\n",
    "        if snap_endpoints:\n",
    "            #### Snap endpoints ####\n",
    "            labelmapVolumeNode = slicer.mrmlScene.AddNewNodeByClass('vtkMRMLLabelMapVolumeNode')\n",
    "            slicer.modules.segmentations.logic().ExportAllSegmentsToLabelmapNode(segmentationNode, labelmapVolumeNode)\n",
    "            segmentationArray = slicer.util.arrayFromVolume(labelmapVolumeNode)\n",
    "            \n",
    "            # Get affine matrix from segmentation labelMapVolumeNode\n",
    "            vtkAff = vtk.vtkMatrix4x4()\n",
    "            aff = np.eye(4)\n",
    "            labelmapVolumeNode.GetIJKToRASMatrix(vtkAff)\n",
    "            vtkAff.DeepCopy(aff.ravel(), vtkAff)\n",
    "            new_endpoints = []\n",
    "            print('check 5')\n",
    "            #endpoints = np.matrix.transpose(np.array(endpointPositions))\n",
    "            for endpoint in endpointPositions:\n",
    "                new_endpoint = utils.robustEndPointDetection(endpoint, segmentationArray, aff, n=5)\n",
    "                new_endpoints.append(new_endpoint)\n",
    "        \n",
    "            endPointsMarkupsNode2 = slicer.mrmlScene.AddNewNodeByClass(\"vtkMRMLMarkupsFiducialNode\", \"Centerline endpoints NEW\")\n",
    "            endPointsMarkupsNode2.GetDisplayNode().PointLabelsVisibilityOff()\n",
    "            endPointsMarkupsNode2.RemoveAllMarkups()\n",
    "            print('check 6')\n",
    "            \n",
    "            for position in new_endpoints:\n",
    "                endPointsMarkupsNode2.AddControlPoint(vtk.vtkVector3d(position))\n",
    "        \n",
    "            if endPointsMarkupsNode2.GetNumberOfControlPoints() > 0:\n",
    "                endPointsMarkupsNode2.SetNthControlPointSelected(0, False)\n",
    "            ####################################################\n",
    "    \n",
    "            centerlinePolyData2, voronoiDiagramPolyData = logic.extractCenterline(preProcess, endPointsMarkupsNode2)\n",
    "            centerlinePropertiesTableNode2 = None\n",
    "    \n",
    "            # Save centerlinePolyData as vtk\n",
    "            centerline_file_path2 = f\"{save_path}/{ID}_centerline2.vtk\"\n",
    "            vtk_writer.SetFileName(centerline_file_path2)\n",
    "            vtk_writer.SetInputData(centerlinePolyData2)\n",
    "            vtk_writer.SetFileVersion(42)\n",
    "            vtk_writer.Write()\n",
    "            print(f\"Saved centerline data to {centerline_file_path2}\")\n",
    "    \n",
    "            # Save endPointsMarkupsNode as fcsv (fiducial list)\n",
    "            endpoints_file_path = f\"{save_path}/{ID}_endpoints2.fcsv\"\n",
    "            slicer.util.saveNode(endPointsMarkupsNode2, endpoints_file_path)\n",
    "            print(f\"Saved endpoints to {endpoints_file_path}\")\n",
    "    \n",
    "            slicer.mrmlScene.RemoveNode(endPointsMarkupsNode2)\n",
    "        \n",
    "    \n",
    "    # Step 6: Remove all loaded nodes at the end\n",
    "    slicer.mrmlScene.RemoveNode(loadedVolume)\n",
    "    slicer.mrmlScene.RemoveNode(labelmapVolumeNode)\n",
    "    slicer.mrmlScene.RemoveNode(segmentationNode)\n",
    "    slicer.mrmlScene.RemoveNode(endPointsMarkupsNode)\n",
    "    slicer.mrmlScene.RemoveNode(labelmapVolumeNode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57f7dbac-d55e-412d-ba15-1a800eb8f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved network data to ./vmtk_out_pa/example_patree_network.vtp\n",
      "Saved endpoints to ./vmtk_out_pa/example_patree_endpoints.fcsv\n",
      "Saved network data to ./vmtk_out_pa/example_patree_meshPreProcessed.vtk\n"
     ]
    }
   ],
   "source": [
    "file_name = './example_patree.nii.gz'\n",
    "extract_pa_tree(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ba1dd-f482-46f5-b5dc-a4d990ad1941",
   "metadata": {},
   "source": [
    "# Trim extracted network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9782d8-d041-41ed-8820-6c6a31fd4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topology(dicts, polydata):\n",
    "    cell_data = polydata.GetCellData()\n",
    "    length_array = cell_data.GetArray('Length')\n",
    "    #LPA or RPA\n",
    "    # the length of the RPA is always greater than that of the LPA\n",
    "    dict1, dict2 = dicts[1], dicts[2]\n",
    "    cell1_id, cell2_id = dict1['cell'], dict2['cell']\n",
    "    cell1, cell2 = polydata.GetCell(cell1_id), polydata.GetCell(cell2_id)\n",
    "    if length_array.GetValue(cell1_id) > length_array.GetValue(cell2_id):\n",
    "        #Assign 1 as RPA\n",
    "        dict1['side'] = 'RPA'\n",
    "        dict2['side'] = 'LPA'\n",
    "    else:\n",
    "        dict1['side'] = 'LPA'\n",
    "        dict2['side'] = 'RPA'\n",
    "\n",
    "    cell_to_dict = {d['cell']: d for d in dicts}\n",
    "    #This loop works on the assumption that the dicts are in order of generation (which they should be if using original code for original task...)\n",
    "    for dict in dicts:\n",
    "        gen_num = dict['gen']\n",
    "        if gen_num >= 2:\n",
    "            cell_id = dict['cell']\n",
    "            parent_dict = cell_to_dict.get(dict['parent'])\n",
    "            parent_side = parent_dict['side']\n",
    "            if parent_side in ['R', 'RPA']:\n",
    "                dict['side'] = 'R'\n",
    "            elif parent_side in ['L', 'LPA']:\n",
    "                dict['side'] = 'L'\n",
    "            else:\n",
    "                raise ValueError('There is a mistake with the side somehow...')\n",
    "                \n",
    "    return dicts\n",
    "\n",
    "def analyse_cells(polydata, cells, column_prefix_name = 'Gen1_R'):\n",
    "    radii = [] # as long as points in cells\n",
    "    tapers = [] # as long as cells\n",
    "    torsions = [] # as long as points in cells (pointArray)\n",
    "    torts = [] # as long as cells (cellArray)\n",
    "    curvs = [] # as long as points in cells (pointArray)\n",
    "    vols = [] # as long as cells\n",
    "    surfs = [] # as long as cells\n",
    "    lengths = [] # as long as cells (cellArray)\n",
    "\n",
    "    cell_data = polydata.GetCellData()\n",
    "    length_array = cell_data.GetArray('Length')\n",
    "    tort_array = cell_data.GetArray('Tortuosity')\n",
    "\n",
    "    point_data = polydata.GetPointData()\n",
    "    curv_array = point_data.GetArray('Curvature')\n",
    "    torsion_array = point_data.GetArray('Torsion')\n",
    "    radius_array = point_data.GetArray('Radius')\n",
    "\n",
    "    points = polydata.GetPoints()\n",
    "\n",
    "    if len(cells) > 0:\n",
    "        for cell_id in cells:\n",
    "            cell = polydata.GetCell(cell_id)\n",
    "    \n",
    "            #Cell info\n",
    "            vol, surf = trim_utils.calc_vol_surf(polydata, cell_id)\n",
    "            vols.append(vol if not np.isnan(vol) else 1e-9)\n",
    "            surfs.append(surf if not np.isnan(surf) else 1e-9)\n",
    "            length = length_array.GetValue(cell_id)\n",
    "            lengths.append(length if not np.isnan(length) else 1e-9)\n",
    "            tort = tort_array.GetValue(cell_id)\n",
    "            torts.append(tort if not np.isnan(tort) else 1e-9)\n",
    "    \n",
    "            point_ids = cell.GetPointIds()\n",
    "            cell_rads = []\n",
    "            for i in range(point_ids.GetNumberOfIds()):\n",
    "                point_id = point_ids.GetId(i)\n",
    "                radius = radius_array.GetValue(point_id)\n",
    "                radii.append(radius if not np.isnan(radius) else 1e-9)\n",
    "                cell_rads.append(radius if not np.isnan(radius) else 1e-9)\n",
    "                torsion = torsion_array.GetValue(point_id)\n",
    "                torsions.append(torsion if not np.isnan(torsion) else 1e-9)\n",
    "                curvature = curv_array.GetValue(point_id)\n",
    "                curvs.append(curvature if not np.isnan(curvature) else 1e-9)\n",
    "    \n",
    "            index_q1 = round(len(cell_rads) * 0.25)\n",
    "            index_q3 = round(len(cell_rads) * 0.75)\n",
    "            taper = 1 - cell_rads[index_q3]/cell_rads[index_q1]\n",
    "            tapers.append(taper)\n",
    "    else:\n",
    "        vols.append(1e-9)\n",
    "        surfs.append(1e-9)\n",
    "        lengths.append(1e-9)\n",
    "        torts.append(1e-9)\n",
    "        radii.append(1e-9)\n",
    "        torsions.append(1e-9)\n",
    "        curvs.append(1e-9)\n",
    "        tapers.append(1)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    #if column_prefix_name == 'Gen4_L':\n",
    "    #    print(radii)\n",
    "    #    print(curvs)\n",
    "    \n",
    "    df[f'{column_prefix_name}_max_radius'] = [max(radii)]\n",
    "    df[f'{column_prefix_name}_avg_radius'] = sum(radii)/len(radii)\n",
    "    df[f'{column_prefix_name}_min_radius'] = min(radii)\n",
    "    df[f'{column_prefix_name}_taper'] = sum(tapers)/len(tapers)\n",
    "    df[f'{column_prefix_name}_max_curvature'] = max(curvs)\n",
    "    df[f'{column_prefix_name}_avg_curvature'] = sum(curvs)/len(curvs)\n",
    "    df[f'{column_prefix_name}_min_curvature'] = min(curvs)\n",
    "    df[f'{column_prefix_name}_max_torsion'] = max(torsions)\n",
    "    df[f'{column_prefix_name}_avg_torsion'] = sum(torsions)/len(torsions)\n",
    "    df[f'{column_prefix_name}_min_torsion'] = min(torsions)\n",
    "    df[f'{column_prefix_name}_total_vol'] = sum(vols)\n",
    "    df[f'{column_prefix_name}_avg_vol'] = sum(vols)/len(vols)\n",
    "    df[f'{column_prefix_name}_total_surf'] = sum(surfs)\n",
    "    df[f'{column_prefix_name}_avg_surf'] = sum(surfs)/len(surfs)\n",
    "    df[f'{column_prefix_name}_total_length'] = sum(lengths)\n",
    "    df[f'{column_prefix_name}_vols_to_length'] = sum(vols)/sum(lengths)    \n",
    "    df[f'{column_prefix_name}_avg_length'] = sum(lengths)/len(lengths)\n",
    "    df[f'{column_prefix_name}_max_tortuosity'] = max(torts)\n",
    "    df[f'{column_prefix_name}_avg_tortuosity'] = sum(torts)/len(torts)\n",
    "    df[f'{column_prefix_name}_min_tortuosity'] = min(torts)\n",
    "\n",
    "    return df \n",
    "\n",
    "def get_metrics(polydata, dicts):\n",
    "    total_vol, total_surf = 0.0, 0.0\n",
    "    for i in range(polydata.GetNumberOfCells()):\n",
    "        v,s = trim_utils.calc_vol_surf(polydata, i)\n",
    "        total_vol += v\n",
    "        total_surf += s\n",
    "        \n",
    "    \n",
    "    gen_to_cells = {}\n",
    "    for dict in dicts:\n",
    "        gen_num = dict['gen']\n",
    "        cell_id = dict['cell']\n",
    "        if gen_num in gen_to_cells:\n",
    "            gen_to_cells[gen_num].append(cell_id)\n",
    "        else:\n",
    "            gen_to_cells[gen_num] = [cell_id]\n",
    "\n",
    "    cell_to_dict = {d['cell']: d for d in dicts}\n",
    "\n",
    "\n",
    "    for i in range(max(gen_to_cells) + 1):\n",
    "    #for i in range(2):\n",
    "        ids = gen_to_cells[i]\n",
    "        if i == 0:\n",
    "            full_df = analyse_cells(polydata, ids, column_prefix_name=f'Gen0')\n",
    "        else:\n",
    "            L_cells, R_cells = [], []\n",
    "            for id in ids:\n",
    "                dict = cell_to_dict[id]\n",
    "                if dict['side'] in ['L','LPA']:\n",
    "                    L_cells.append(id)\n",
    "                elif dict['side'] in ['R', 'RPA']:\n",
    "                    R_cells.append(id)\n",
    "                else:\n",
    "                    raise ValueError('Something has gone wrong...')\n",
    "            print(f'analysing Gen {i}')\n",
    "            print(L_cells)\n",
    "            L_df = analyse_cells(polydata, L_cells, column_prefix_name = f'Gen{i}_L')\n",
    "            R_df = analyse_cells(polydata, R_cells, column_prefix_name = f'Gen{i}_R')\n",
    "\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            # L vs R\n",
    "            # Vol ratio, surf ratio, max radius, length ratio, average tortuosity ratio\n",
    "            df[f'Gen{i}_LR_total_vol_ratio'] = [L_df[f'Gen{i}_L_total_vol'][0]/R_df[f'Gen{i}_R_total_vol'][0]]\n",
    "            df[f'Gen{i}_LR_total_surf_ratio'] = L_df[f'Gen{i}_L_total_surf'][0]/R_df[f'Gen{i}_R_total_surf'][0]\n",
    "            df[f'Gen{i}_LR_max_radius_ratio'] = L_df[f'Gen{i}_L_max_radius'][0]/R_df[f'Gen{i}_R_max_radius'][0]\n",
    "            df[f'Gen{i}_LR_length_ratio'] = L_df[f'Gen{i}_L_total_length'][0]/R_df[f'Gen{i}_R_total_length'][0]\n",
    "            df[f'Gen{i}_LR_avg_tortuosity_ratio'] = L_df[f'Gen{i}_L_avg_tortuosity'][0]/R_df[f'Gen{i}_R_avg_tortuosity'][0]\n",
    "            \n",
    "            # Get max/min/avgs of L and R combined\n",
    "            df[f'Gen{i}_max_radius'] = [max(L_df[f'Gen{i}_L_max_radius'][0], R_df[f'Gen{i}_R_max_radius'][0])]\n",
    "            df[f'Gen{i}_avg_radius'] = (L_df[f'Gen{i}_L_avg_radius'][0] + R_df[f'Gen{i}_R_avg_radius'][0])/2\n",
    "            df[f'Gen{i}_min_radius'] = min(L_df[f'Gen{i}_L_min_radius'][0], R_df[f'Gen{i}_R_min_radius'][0])\n",
    "            df[f'Gen{i}_taper'] = (L_df[f'Gen{i}_L_taper'][0] + R_df[f'Gen{i}_R_taper'][0])/2\n",
    "            df[f'Gen{i}_max_curvature'] = max(L_df[f'Gen{i}_L_max_curvature'][0], R_df[f'Gen{i}_R_max_curvature'][0])\n",
    "            df[f'Gen{i}_avg_curvature'] = (L_df[f'Gen{i}_L_avg_curvature'][0] + R_df[f'Gen{i}_R_avg_curvature'][0])/2\n",
    "            df[f'Gen{i}_min_curvature'] = min(L_df[f'Gen{i}_L_min_curvature'][0], R_df[f'Gen{i}_R_min_curvature'][0])\n",
    "            df[f'Gen{i}_max_torsion'] = max(L_df[f'Gen{i}_L_max_torsion'][0], R_df[f'Gen{i}_R_max_torsion'][0])\n",
    "            df[f'Gen{i}_avg_torsion'] = (L_df[f'Gen{i}_L_avg_torsion'][0] + R_df[f'Gen{i}_R_avg_torsion'][0])/2\n",
    "            df[f'Gen{i}_min_torsion'] = min(L_df[f'Gen{i}_L_min_torsion'][0], R_df[f'Gen{i}_R_min_torsion'][0])\n",
    "            df[f'Gen{i}_total_vol'] = L_df[f'Gen{i}_L_total_vol'][0] + R_df[f'Gen{i}_R_total_vol'][0]\n",
    "            df[f'Gen{i}_vol_to_whole_vol'] = (L_df[f'Gen{i}_L_total_vol'][0] + R_df[f'Gen{i}_R_total_vol'][0])/total_vol\n",
    "            df[f'Gen{i}_avg_vol'] = (L_df[f'Gen{i}_L_avg_vol'][0] + R_df[f'Gen{i}_R_avg_vol'][0])/2\n",
    "            df[f'Gen{i}_total_surf'] = L_df[f'Gen{i}_L_total_surf'][0] + R_df[f'Gen{i}_R_total_surf'][0]\n",
    "            df[f'Gen{i}_surf_to_whole_surf'] = (L_df[f'Gen{i}_L_total_surf'][0] + R_df[f'Gen{i}_R_total_surf'][0])/total_surf\n",
    "            df[f'Gen{i}_avg_surf'] = (L_df[f'Gen{i}_L_avg_surf'][0] + R_df[f'Gen{i}_R_avg_surf'][0])/2\n",
    "            df[f'Gen{i}_total_length'] = L_df[f'Gen{i}_L_total_length'][0] + R_df[f'Gen{i}_R_total_length'][0]\n",
    "            df[f'Gen{i}_avg_length'] = (L_df[f'Gen{i}_L_avg_length'][0] + R_df[f'Gen{i}_R_avg_length'][0])/2\n",
    "            df[f'Gen{i}_vols_to_length'] = (L_df[f'Gen{i}_L_total_vol'][0] + R_df[f'Gen{i}_R_total_vol'][0])/(L_df[f'Gen{i}_L_total_length'][0] + R_df[f'Gen{i}_R_total_length'][0])     \n",
    "            df[f'Gen{i}_max_tortuosity'] = max(L_df[f'Gen{i}_L_max_tortuosity'][0], R_df[f'Gen{i}_R_max_tortuosity'][0])\n",
    "            df[f'Gen{i}_avg_tortuosity'] = (L_df[f'Gen{i}_L_avg_tortuosity'][0] + R_df[f'Gen{i}_R_avg_tortuosity'][0])/2\n",
    "            df[f'Gen{i}_min_tortuosity'] = min(L_df[f'Gen{i}_L_min_tortuosity'][0], R_df[f'Gen{i}_R_min_tortuosity'][0])\n",
    "            # Gen i-1 vs Gen i\n",
    "            # i-1 vol / i vol\n",
    "            # i-1 surf/ i surf\n",
    "            # i-1 max radius / i max radius\n",
    "            # i-1 length / i length\n",
    "            # \n",
    "            df[f'Gen{i-1}_Gen{i}_total_vol_ratio'] = full_df[f'Gen{i-1}_total_vol'][0]/df[f'Gen{i}_total_vol'][0]\n",
    "            df[f'Gen{i-1}_Gen{i}_total_surf_ratio'] = full_df[f'Gen{i-1}_total_surf'][0]/df[f'Gen{i}_total_surf'][0]\n",
    "            df[f'Gen{i-1}_Gen{i}_avg_vol_ratio'] = full_df[f'Gen{i-1}_avg_vol'][0]/df[f'Gen{i}_avg_vol'][0]\n",
    "            df[f'Gen{i-1}_Gen{i}_avg_surf_ratio'] = full_df[f'Gen{i-1}_avg_surf'][0]/df[f'Gen{i}_avg_surf'][0]\n",
    "            df[f'Gen{i-1}_Gen{i}_max_radius_ratio'] = full_df[f'Gen{i-1}_max_radius'][0]/df[f'Gen{i}_max_radius'][0]\n",
    "            df[f'Gen{i-1}_Gen{i}_avg_radius_ratio'] = full_df[f'Gen{i-1}_avg_radius'][0]/df[f'Gen{i}_avg_radius'][0]\n",
    "            df[f'Gen{i-1}_Gen{i}_total_length_ratio'] = full_df[f'Gen{i-1}_total_length'][0]/df[f'Gen{i}_total_length'][0]\n",
    "            df[f'Gen{i-1}_Gen{i}_avg_length_ratio'] = full_df[f'Gen{i-1}_avg_length'][0]/df[f'Gen{i}_avg_length'][0]\n",
    "\n",
    "            full_df = pd.concat([full_df, L_df, R_df, df], axis=1)\n",
    "\n",
    "    #Gen (0 and 1) vs total\n",
    "    df = pd.DataFrame()\n",
    "    mpalparpa_vol = full_df[f'Gen0_total_vol'][0] + full_df[f'Gen1_total_vol'][0]\n",
    "    df[f'Gen01_Gen2Plus_vol'] = [mpalparpa_vol / (total_vol - mpalparpa_vol)]\n",
    "\n",
    "    full_df = pd.concat([full_df, df], axis=1)\n",
    "            \n",
    "    return full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9df1d0af-0c95-45bb-b4f4-408f3d70d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_network_pa_tree(file_name, n_bifurcation):\n",
    "    # Input: file_name = 'base_path/ID.nii.gz'\n",
    "    # Saves \n",
    "    logic = slicer_utils.ExtractCenterlineLogic()\n",
    "    \n",
    "    base_name = os.path.basename(file_name)\n",
    "    ID = base_name.split('.nii.gz')[0]\n",
    "    base_path = file_name.split(ID)[0]\n",
    "    vtk_folder = os.path.join(base_path,'vmtk_out_pa')\n",
    "    save_path = os.path.join(vtk_folder, f'{n_bifurcation}_out/')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    #\n",
    "    network_path = os.path.join(vtk_folder, f'{ID}_network.vtp')\n",
    "    network = read_vtpfile(network_path)\n",
    "\n",
    "    network_save_file = os.path.join(save_path,f'{ID}_trimmed_net.vtp')\n",
    "    root_save_file = os.path.join(save_path,f'{ID}_root.vtp')\n",
    "    dicts_file = os.path.join(save_path, f'{ID}_dicts.json')\n",
    "    \n",
    "    if os.path.exists(network_save_file):\n",
    "        trimmed_net = read_vtpfile(network_save_file)\n",
    "        print(f'Reading file {network_save_file}')\n",
    "        \n",
    "        with open(dicts_file, 'r') as json_file:\n",
    "            dicts = json.load(json_file)\n",
    "    else:\n",
    "        source_cell = trim_utils.find_root_cellID_from_network(network)\n",
    "        trimmed_net, dicts = trim_utils.reconstruct_network_with_cell_data(network, source_cell, n_bifurcation)\n",
    "        write_vtpfile(trimmed_net, network_save_file)\n",
    "\n",
    "        root_net = trim_utils.extract_single_cell_as_polydata(network, source_cell)\n",
    "        write_vtpfile(root_net, root_save_file)\n",
    "        \n",
    "        with open(dicts_file, 'w') as json_file:\n",
    "                json.dump(dicts, json_file)\n",
    "            \n",
    "\n",
    "    dicts = assign_topology(dicts, network)\n",
    "    \n",
    "    id_df = get_metrics(network, dicts)\n",
    "    id_df.index = [ID]\n",
    "\n",
    "    calculate_centerlines = False\n",
    "    if calculate_centerlines:\n",
    "        preprocessed_file_name = os.path.join(vtk_folder, f'{ID}_meshPreProcessed.vtk')\n",
    "        preProcess = read_vtkfile(preprocessed_file_name)\n",
    "        \n",
    "        startPointPosition=None\n",
    "        endPointsMarkupsNode = slicer.mrmlScene.AddNewNodeByClass(\"vtkMRMLMarkupsFiducialNode\", \"Centerline endpoints\")\n",
    "        endpointPositions = logic.getEndPoints(trimmed_net, startPointPosition)\n",
    "        endPointsMarkupsNode.GetDisplayNode().PointLabelsVisibilityOff()\n",
    "        endPointsMarkupsNode.RemoveAllMarkups()\n",
    "    \n",
    "        \n",
    "        for position in endpointPositions:\n",
    "            endPointsMarkupsNode.AddControlPoint(vtk.vtkVector3d(position))\n",
    "    \n",
    "        if endPointsMarkupsNode.GetNumberOfControlPoints() > 0:\n",
    "            endPointsMarkupsNode.SetNthControlPointSelected(0, False)\n",
    "        ##########\n",
    "\n",
    "        if not endPointsMarkupsNode or endPointsMarkupsNode.GetNumberOfControlPoints() < 2:\n",
    "            print('*****Two end points are needed for centerline extraction!!!******')\n",
    "            print(f'Skipping ID = {ID}. Best to do manually.')\n",
    "            slicer.mrmlScene.RemoveNode(endPointsMarkupsNode)\n",
    "    \n",
    "            return 0\n",
    "            \n",
    "        utils_logic = utils.ExtractCenterlineLogic()\n",
    "        centerlinePolyData, voronoiDiagramPolyData = utils_logic.extractCenterline(preProcess, endPointsMarkupsNode)\n",
    "        centerlinePropertiesTableNode = None\n",
    "\n",
    "        snap_endpoints = True\n",
    "        if snap_endpoints:\n",
    "            \n",
    "            referenceVolume = slicer.util.loadVolume(file_name)\n",
    "            #### Snap endpoints ####            \n",
    "            # Get affine matrix from segmentation labelMapVolumeNode\n",
    "            vtkAff = vtk.vtkMatrix4x4()\n",
    "            aff = np.eye(4)\n",
    "            #labelmapVolumeNode.GetIJKToRASMatrix(vtkAff)\n",
    "            referenceVolume.GetIJKToRASMatrix(vtkAff)\n",
    "            vtkAff.DeepCopy(aff.ravel(), vtkAff)\n",
    "            new_endpoints = []\n",
    "            print('check 5')\n",
    "            #endpoints = np.matrix.transpose(np.array(endpointPositions))\n",
    "\n",
    "            segmentationArray = slicer.util.arrayFromVolume(referenceVolume)\n",
    "            for endpoint in endpointPositions:\n",
    "                new_endpoint = utils.robustEndPointDetection(endpoint, segmentationArray, aff, n=5)\n",
    "                new_endpoints.append(new_endpoint)\n",
    "        \n",
    "            endPointsMarkupsNode2 = slicer.mrmlScene.AddNewNodeByClass(\"vtkMRMLMarkupsFiducialNode\", \"Centerline endpoints NEW\")\n",
    "            endPointsMarkupsNode2.GetDisplayNode().PointLabelsVisibilityOff()\n",
    "            endPointsMarkupsNode2.RemoveAllMarkups()\n",
    "            print('check 6')\n",
    "            \n",
    "            for position in new_endpoints:\n",
    "                endPointsMarkupsNode2.AddControlPoint(vtk.vtkVector3d(position))\n",
    "        \n",
    "            if endPointsMarkupsNode2.GetNumberOfControlPoints() > 0:\n",
    "                endPointsMarkupsNode2.SetNthControlPointSelected(0, False)\n",
    "            ####################################################\n",
    "    \n",
    "            centerlinePolyData2, voronoiDiagramPolyData = utils_logic.extractCenterline(preProcess, endPointsMarkupsNode2)\n",
    "            centerlinePropertiesTableNode2 = None\n",
    "    \n",
    "            # Save centerlinePolyData as vtk\n",
    "            centerline_file_path2 = f\"{save_path}/{ID}_centerline2.vtk\"\n",
    "            write_vtkfile(centerlinePolyData2,centerline_file_path2)\n",
    "            print(f\"Saved centerline data to {centerline_file_path2}\")\n",
    "    \n",
    "            # Save endPointsMarkupsNode as fcsv (fiducial list)\n",
    "            endpoints_file_path = f\"{save_path}/{ID}_endpoints2.fcsv\"\n",
    "            slicer.util.saveNode(endPointsMarkupsNode2, endpoints_file_path)\n",
    "            print(f\"Saved endpoints to {endpoints_file_path}\")\n",
    "    \n",
    "            #slicer.mrmlScene.RemoveNode(endPointsMarkupsNode2)\n",
    "        centerline_file_path = f\"{save_path}/{ID}_centerline2.vtk\"\n",
    "        write_vtkfile(centerlinePolyData,centerline_file_path)\n",
    "        print(f\"Saved centerline data to {centerline_file_path}\")\n",
    "\n",
    "        # Save endPointsMarkupsNode as fcsv (fiducial list)\n",
    "        endpoints_file_path = f\"{save_path}/{ID}_endpoints.fcsv\"\n",
    "        slicer.util.saveNode(endPointsMarkupsNode, endpoints_file_path)\n",
    "        print(f\"Saved endpoints to {endpoints_file_path}\")\n",
    "\n",
    "    return id_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d6c184-055e-4a1b-86b9-33fb39ee6372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./vmtk_out_pa\\5_out/example_patree_trimmed_net.vtp\n",
      "analysing Gen 1\n",
      "[104]\n",
      "analysing Gen 2\n",
      "[77, 103]\n",
      "analysing Gen 3\n",
      "[62, 76, 134, 135, 136, 137]\n",
      "analysing Gen 4\n",
      "[49, 61, 63, 64, 101, 102, 161, 162, 163, 164, 165, 166, 167, 168]\n",
      "analysing Gen 5\n",
      "[34, 48, 73, 74, 75, 78, 79, 80, 81, 131, 753, 132, 133, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]\n"
     ]
    }
   ],
   "source": [
    "max_bifurcations = 5\n",
    "id_df = trim_network_pa_tree(file_name, max_bifurcations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f14058-ae7f-4cb7-aa1a-44c59ab4fe6e",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7c2a65-16a2-4fe9-87c3-07ae522012cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = read_vtkfile('./vmtk_out_pa/example_patree_meshPreProcessed.vtk')\n",
    "net = read_vtpfile('./vmtk_out_pa/example_patree_network.vtp')\n",
    "root = read_vtpfile('./vmtk_out_pa/5_out/example_patree_root.vtp')\n",
    "trimmed_net = read_vtpfile('./vmtk_out_pa/5_out/example_patree_trimmed_net.vtp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a69bb7-48bf-457b-8c9f-19b7404a43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer_utils.visualise_slicer(mesh,'mesh')\n",
    "slicer_utils.visualise_slicer(net,'net')\n",
    "slicer_utils.visualise_slicer(root,'root')\n",
    "slicer_utils.visualise_slicer(trimmed_net,'trimmed_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122edbe-026b-4bd6-9943-3ba24a1c1561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Slicer 5.6",
   "language": "python",
   "name": "slicer-5.6"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
